batch: 0
Loss: 6.948692798614502
batch: 1
Loss: 6.947112083435059
batch: 2
Loss: 6.9223785400390625
batch: 3
Loss: 6.936032295227051
batch: 4
Loss: 6.948981285095215
batch: 5
Loss: 6.955179691314697
batch: 6
Loss: 6.967916488647461
batch: 7
Loss: 6.960204124450684
batch: 8
Loss: 6.922911643981934
batch: 9
Loss: 6.926852226257324
batch: 10
Loss: 6.959813594818115
batch: 11
Loss: 6.939917087554932
batch: 12
Loss: 6.923253059387207
batch: 13
Loss: 6.944328784942627
batch: 14
Loss: 6.939859390258789
batch: 15
Loss: 6.919595718383789
batch: 16
Loss: 6.930741310119629
batch: 17
Loss: 6.934266090393066
batch: 18
Loss: 6.950953483581543
batch: 19
Loss: 6.936223983764648
batch: 20
Loss: 6.908068656921387
batch: 21
Loss: 6.91484260559082
batch: 22
Loss: 6.928657054901123
batch: 23
Loss: 6.938316822052002
batch: 24
Loss: 6.943517684936523
batch: 25
Loss: 6.924009799957275
batch: 26
Loss: 6.899230003356934
batch: 27
Loss: 6.9128217697143555
batch: 28
Loss: 6.909611225128174
batch: 29
Loss: 6.930426120758057
batch: 30
Loss: 6.914699554443359
batch: 31
Loss: 6.92254638671875
batch: 32
Loss: 6.905233383178711
batch: 33
Loss: 6.906907558441162
batch: 34
Loss: 6.916231155395508
batch: 35
Loss: 6.915276527404785
batch: 36
Loss: 6.914254665374756
batch: 37
Loss: 6.916958332061768
batch: 38
Loss: 6.921648979187012
batch: 39
Loss: 6.922684669494629
batch: 40
Loss: 6.913147449493408
batch: 41
Loss: 6.898360729217529
batch: 42
Loss: 6.92415189743042
batch: 43
Loss: 6.909085273742676
batch: 44
Loss: 6.905589580535889
batch: 45
Loss: 6.8950514793396
batch: 46
Loss: 6.928207874298096
batch: 47
Loss: 6.92473030090332
batch: 48
Loss: 6.917389392852783
batch: 49
Loss: 6.920069217681885
batch: 50
Loss: 6.907963275909424
batch: 51
Loss: 6.889913082122803
batch: 52
Loss: 6.90521240234375
batch: 53
Loss: 6.904942512512207
batch: 54
Loss: 6.8998212814331055
batch: 55
Loss: 6.904629230499268
batch: 56
Loss: 6.898165225982666
batch: 57
Loss: 6.886895179748535
batch: 58
Loss: 6.89702033996582
batch: 59
Loss: 6.886051654815674
batch: 60
Loss: 6.8961029052734375
batch: 61
Loss: 6.891329288482666
batch: 62
Loss: 6.887503147125244
batch: 63
Loss: 6.910764694213867
batch: 64
Loss: 6.887418270111084
batch: 65
Loss: 6.8777546882629395
batch: 66
Loss: 6.92402458190918
batch: 67
Loss: 6.891198635101318
batch: 68
Loss: 6.8900041580200195
batch: 69
Loss: 6.88498067855835
batch: 70
Loss: 6.898238658905029
batch: 71
Loss: 6.8655877113342285
batch: 72
Loss: 6.890425205230713
batch: 73
Loss: 6.883898735046387
batch: 74
Loss: 6.88911247253418
batch: 75
Loss: 6.886007785797119
batch: 76
Loss: 6.871029376983643
batch: 77
Loss: 6.873398780822754
batch: 78
Loss: 6.892990589141846
batch: 79
Loss: 6.890493869781494
batch: 80
Loss: 6.8747992515563965
batch: 81
Loss: 6.861894607543945
batch: 82
Loss: 6.854198455810547
batch: 83
Loss: 6.881668567657471
batch: 84
Loss: 6.87760591506958
batch: 85
Loss: 6.878788948059082
batch: 86
Loss: 6.872419357299805
batch: 87
Loss: 6.871005535125732
batch: 88
Loss: 6.876025199890137
batch: 89
Loss: 6.883910655975342
batch: 90
Loss: 6.882561683654785
batch: 91
Loss: 6.853457450866699
batch: 92
Loss: 6.870297431945801
batch: 93
Loss: 6.852712154388428
batch: 94
Loss: 6.882744312286377
batch: 95
Loss: 6.89201021194458
batch: 96
Loss: 6.888489723205566
batch: 97
Loss: 6.870863914489746
batch: 98
Loss: 6.858567237854004
batch: 99
Loss: 6.86273193359375
batch: 100
Loss: 6.847469806671143
batch: 101
Loss: 6.885847568511963
batch: 102
Loss: 6.868471145629883
batch: 103
Loss: 6.858231544494629
batch: 104
Loss: 6.853272914886475
batch: 105
Loss: 6.860535144805908
batch: 106
Loss: 6.84796142578125
batch: 107
Loss: 6.849878311157227
batch: 108
Loss: 6.860490322113037
batch: 109
Loss: 6.858102798461914
batch: 110
Loss: 6.838942050933838
batch: 111
Loss: 6.82877254486084
batch: 112
Loss: 6.855446815490723
batch: 113
Loss: 6.843287944793701
batch: 114
Loss: 6.853736877441406
batch: 115
Loss: 6.849565029144287
batch: 116
Loss: 6.8351054191589355
batch: 117
Loss: 6.829315662384033
batch: 118
Loss: 6.8424835205078125
batch: 119
Loss: 6.833261013031006
batch: 120
Loss: 6.825525283813477
batch: 121
Loss: 6.8576507568359375
batch: 122
Loss: 6.833865642547607
batch: 123
Loss: 6.837520599365234
batch: 124
Loss: 6.846103191375732
batch: 125
Loss: 6.845010280609131
batch: 126
Loss: 6.834657192230225
batch: 127
Loss: 6.825109004974365
batch: 128
Loss: 6.852776527404785
batch: 129
Loss: 6.818150043487549
batch: 130
Loss: 6.833940029144287
batch: 131
Loss: 6.82559061050415
batch: 132
Loss: 6.8282976150512695
batch: 133
Loss: 6.8034892082214355
batch: 134
Loss: 6.832461833953857
batch: 135
Loss: 6.825369358062744
batch: 136
Loss: 6.852939128875732
batch: 137
Loss: 6.8280768394470215
batch: 138
Loss: 6.837796211242676
batch: 139
Loss: 6.793164253234863
batch: 140
Loss: 6.827503204345703
batch: 141
Loss: 6.810074329376221
batch: 142
Loss: 6.818456649780273
batch: 143
Loss: 6.808162212371826
batch: 144
Loss: 6.816538333892822
batch: 145
Loss: 6.796339988708496
batch: 146
Loss: 6.842130184173584
batch: 147
Loss: 6.814043998718262
batch: 148
Loss: 6.799771785736084
batch: 149
Loss: 6.831049919128418
batch: 150
Loss: 6.8227667808532715
batch: 151
Loss: 6.80684232711792
batch: 152
Loss: 6.8040056228637695
batch: 153
Loss: 6.820674896240234
batch: 154
Loss: 6.781336307525635
batch: 155
Loss: 6.807015895843506
batch: 156
Loss: 6.819029808044434
batch: 157
Loss: 6.789971828460693
batch: 158
Loss: 6.795481204986572
batch: 159
Loss: 6.801028251647949
batch: 160
Loss: 6.8023834228515625
batch: 161
Loss: 6.79841423034668
batch: 162
Loss: 6.803582191467285
batch: 163
Loss: 6.79303503036499
Traceback (most recent call last):
  File "main.py", line 75, in <module>
    main()
  File "main.py", line 54, in main
    ResNet = train(
  File "C:\Users\jthan\OneDrive\Desktop\2024\Projects\Bird_API\src\train.py", line 15, in train
    for i_batch, sample_batched in enumerate(dataloader):
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 214, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [3, 128, 128] at entry 0 and [4, 128, 128] at entry 114
Traceback (most recent call last):
  File "main.py", line 75, in <module>
    main()
  File "main.py", line 54, in main
    ResNet = train(
  File "C:\Users\jthan\OneDrive\Desktop\2024\Projects\Bird_API\src\train.py", line 15, in train
    for i_batch, sample_batched in enumerate(dataloader):
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\fetch.py", line 55, in fetch
    return self.collate_fn(data)
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 317, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 174, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 174, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 142, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
  File "C:\Users\jthan\anaconda3\envs\cs7643-a4\lib\site-packages\torch\utils\data\_utils\collate.py", line 214, in collate_tensor_fn
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [3, 128, 128] at entry 0 and [4, 128, 128] at entry 114
